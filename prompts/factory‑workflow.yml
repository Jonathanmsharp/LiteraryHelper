# Factory Workflow
# Follow each step in order. You will find rule specs under /config/rules.json and prompt templates in /config/ai-prompts/.

Step 1: Initialise GitHub Repository
Make sure private GitHub repo named LiteraryHelper is set up and ready for development;

Step 2: Configure Vercel Project
Make sure the GitHub repo is linked properly to Vercel, enabling automatic deployments for main (production) and PR branches (preview).

Step 3: Scaffold Monorepo with Turbo & PNPM
Set up a Turborepo workspace using PNPM with two apps (apps/frontend, apps/backend) and shared config (packages/config). Enable TypeScript project references.

Step 4: Add Dev‑Tooling Baseline
Configure ESLint, Prettier, Husky pre‑commit hooks (lint + test); enforce format on commit.

Step 5: Bootstrap Frontend with Next.js + TS
Inside apps/frontend run npx create-next-app@latest --ts; remove boilerplate pages and commit.

Step 6: Install Slate.js & Editor Dependencies
Add slate, slate-react, slate-history, zustand, react-hotkeys-hook. Commit empty Editor component.

Step 7: Optimise Editor Rendering
Implement Slate editor with React.memo, custom leaf/element renderers, Web Worker offload for syntax highlighting, and 500 ms on‑change debounce.

Step 8: Scaffold Backend with Express + TS
Inside apps/backend run npm init -y; install express, bullmq, ioredis, pg, openai, zod, jsonwebtoken, ts-node-dev, typescript. Generate tsconfig.json.

Step 9: Define Shared Types Package
Create packages/types with TypeScript interfaces for Rule, AnalysisRequest, AnalysisResult.

Step 10: Implement PostgreSQL Schema Migration
Add drizzle or knex migration scripts creating tables rules, users, analyses, plus JSONB rule metadata.

Step 11: Provision Local DB & Cache
Add Docker Compose services for postgres and redis; expose ports 5432 and 6379. Commit.

Step 12: Implement Rule Base Class
In backend, create abstract Rule class with id, severity, and async execute(text).

Step 13: Implement SimpleRuleProcessor
Create processor using spaCy.js patterns for Rules 1‑3; load patterns from rules.json.

Step 14: Implement AIRuleProcessor
Create processor that accepts provider and model params; default to openai:gpt‑4o. Build per‑rule prompt templates.

Step 15: Implement Bull Queue Infrastructure
Instantiate Bull queue analysis; workers: simpleRuleWorker (5 concurrency) and aiRuleWorker (2). Use back‑off and rate‑limits from env.

Step 16: Build Express REST API
Routes: POST /analyze → enqueue job & return job ID; GET /analysis/:id → poll result; GET /rules → rule metadata; POST /feedback.

Step 17: Add WebSocket Gateway
Use ws to push incremental rule results back to client (analysis_progress events).

Step 18: Add Auth Middleware
Implement JWT verify (RS256 public key) and attach req.user; block if invalid or over rate limit.

Step 19: Write Rule Orchestrator Service
Upon job start, run simple rules synchronously, emit first packet; enqueue AI‑rules in parallel; aggregate partial results.

Step 20: Integrate Redis Cache Layer
Wrap analysis results with cache‑aside TTL: 15 min for text‑hash hits; invalidate on text change.

Step 21: Implement Frontend API Client
Create useAnalysis hook: POST text, subscribe WebSocket, merge incremental results into Zustand store.

Step 22: Add Real‑Time Highlighting
Update editor to underline text ranges per rule severity colours; clicking opens sidebar with details.

Step 23: Build Rule Sidebar UI
Design sliding panel listing rule explanation, examples, and “mark as helpful” feedback button.

Step 24: Store User Feedback
On sidebar feedback, POST to /feedback; persist to analysis_feedback table for future model fine‑tuning.

Step 25: Unit Tests – Utilities & Rules
Configure Jest + ts‑jest; write tests for string utilities and each simple rule achieving ≥90 % coverage.

Step 26: Integration Tests – API Flow
With Supertest, cover /analyze happy path, rate‑limit failure, and JWT auth rejection.

Step 27: End‑to‑End Tests on Browserbase MCP
Write Playwright scripts simulating a user typing, triggering analysis, and verifying highlights. Run against Vercel preview via Browserbase.

Step 28: Accessibility Tests
Integrate axe-core into Playwright run; fail pipeline if WCAG‑AA issues detected.

Step 29: Performance & Load Testing
Author Artillery scenario: 50 concurrent users, 1 k‑word docs, 5 min soak; assert P95 < 3 s.

Step 30: GitHub Actions CI Pipeline
Create .github/workflows/ci.yml: install, lint, unit + integration tests, build apps, cache Docker layer.

Step 31: GitHub Actions CD Pipeline
Create .github/workflows/cd.yml: on main success, deploy frontend & backend to Vercel using vercel pull + vercel deploy --prod.

Step 32: Seed Rule Metadata
Add scripts/seedRules.ts inserting six initial rules with patterns, prompt templates, severity, colours.

Step 33: Environment Configuration
Add .vercel/project.json & local .env.example for OPENAI_API_KEY, LLM_PROVIDER, POSTGRES_URL, REDIS_URL, JWT_PUBLIC_KEY.

Step 34: Monitoring Setup
Install prom-client; expose /metrics with queue depth, rule timing, GPT latency; auto‑scraped by Vercel Observability.

Step 35: Logging & Error Handling
Pipe pino logs in JSON; configure global error middleware returning problem‑details (JSON).

Step 36: Security Hardening
Add Helmet for HTTP headers; enforce TLS in Vercel; configure Redis password & renamed dangerous commands.

Step 37: SLA Dashboards
Create Next.js /admin/dashboard page showing P95 rule timings and error rates via /metrics API.

Step 38: Documentation
Generate docs/ARCHITECTURE.md, docs/API.md, docs/RULES.md summarising system; link citations back to plan.

Step 39: Final Smoke Test
Deploy latest main to production; run Browserbase MCP script; ensure green passes on highlights, latency, and auth.

Step 40: Handoff to factory.ai
Commit prompts/factory‑workflow.yml describing repo structure, build, test, and deploy commands; notify stakeholders that the automated agent can now iterate.
